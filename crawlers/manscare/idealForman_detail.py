from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from django.db.models.expressions import result
from seleniumbase import SB
from bs4 import BeautifulSoup
from time import sleep
import time

# ìƒí’ˆ ìƒì„¸ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸í™”

def crawl_product_info() :
    url = "https://www.oliveyoung.co.kr/store/display/getBrandShopDetail.do?onlBrndCd=A001643&t_page=%EC%83%81%ED%92%88%EC%83%81%EC%84%B8&t_click=%EB%B8%8C%EB%9E%9C%EB%93%9C%EA%B4%80_%EC%83%81%EB%8B%A8&t_brand_name=%EC%95%84%EC%9D%B4%EB%94%94%EC%96%BC%ED%8F%AC%EB%A7%A8"

    options = Options()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get(url)

    time.sleep(2)  # JS ë Œë”ë§ ëŒ€ê¸°
    li_elements = driver.find_elements(By.CSS_SELECTOR, 'li[data-goods-idx]')
    product_links = []  # ë§í¬ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    # 1ë²ˆ í˜ì´ì§€
    for li in li_elements:
        a_tag = li.find_element(By.CSS_SELECTOR, 'a')
        href = a_tag.get_attribute('href')
        print(href)
        print("======================================================================")
        if href:
            product_links.append(href)
    # 2~3ë²ˆ í˜ì´ì§€
    for page_no in range(2, 4):
        print(f"\nâœ… {page_no}í˜ì´ì§€ í´ë¦­ ì‹œë„")

        # í˜ì´ì§€ ë²„íŠ¼ í´ë¦­ (JavaScript í´ë¦­)
        try:
            page_btn = driver.find_element(By.CSS_SELECTOR, f'a[data-page-no="{page_no}"]')
            driver.execute_script("arguments[0].click();", page_btn)
        except:
            print(f"âŒ {page_no}í˜ì´ì§€ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨")
            continue

        # í˜ì´ì§€ ì „ìš© ìƒí’ˆì´ ë“±ì¥í•  ë•Œê¹Œì§€ ëŒ€ê¸° (ê°„ë‹¨í•œ ë°©ì‹)
        time.sleep(2)

        # ìƒí’ˆ lië“¤ ë‹¤ì‹œ ì¶”ì¶œ
        product_items = driver.find_elements(By.CSS_SELECTOR, 'li[data-goods-idx]')
        print(f"ğŸ” ìƒí’ˆ ìˆ˜: {len(product_items)}")

        for item in product_items:
            a_tag = item.find_element(By.TAG_NAME, 'a')
            href = a_tag.get_attribute("href")
            product_links.append(href)

    # ì°½ì„ ë‹«ì§€ ì•Šê³  ëŒ€ê¸°
    input("ğŸ‘‰ Enter í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì°½ì´ ë‹«í™ë‹ˆë‹¤...")

    # í•„ìš”ì‹œ ìˆ˜ë™ ì¢…ë£Œ
    driver.quit()
    return product_links

def crawl_product_detail(list) :
    product_data = []
    for url in list :
        print(url)
        with SB(uc=True, test=True) as sb:
            sb.uc_open_with_reconnect(url, reconnect_time=60)

            html = sb.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')

            try:
                # ë¸Œëœë“œ ëª…
                brand_name = sb.get_text("#moveBrandShop")
                print("ë¸Œëœë“œëª… :", brand_name)

                # ì œí’ˆëª…
                product_name = sb.get_text("p.prd_name")
                print("ìƒí’ˆëª… :", product_name)

                # í• ì¸ê°€
                discount_price = sb.get_text("span.price-2 strong")
                print("í• ì¸ê°€ :", discount_price)

                # ì •ê°€
                if sb.is_element_present("span.price-1 strike"):
                    origin_price = sb.get_text("span.price-1 strike")
                    print("ì •ê°€:", origin_price)
                else:
                    origin_price = discount_price
                    print("ì •ê°€ -> í• ì¸ê°€ ëŒ€ì²´:", origin_price)

                # ì„¸ì¼í”Œë˜ê·¸
                flags = []
                span_elements = sb.find_elements("css selector", "p#icon_area span")
                for span in span_elements:
                    flags.append(span.text.strip())

                print("ìƒí’ˆ í”Œë˜ê·¸ ë¦¬ìŠ¤íŠ¸:", flags)

            except Exception as e:
                print("ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨:", e)

            # êµ¬ë§¤ì •ë³´ í´ë¦­
            try:
                sb.click("a.goods_buyinfo")
                sleep(2)
                print("âœ… êµ¬ë§¤ì •ë³´ íƒ­ í´ë¦­ ì™„ë£Œ")

                # ì „ì²´ <dl> ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                dl_elements = sb.find_elements("css selector", "dl.detail_info_list")

                # ê°€ì ¸ì˜¬ ì¸ë±ìŠ¤ (1, 2, 7ë²ˆì§¸ â†’ íŒŒì´ì¬ ê¸°ì¤€: 0, 1, 6)
                target_indices = [1, 2, 7]
                target_fields = ['capacity', 'detail', 'ingredients']  # ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”

                result = {}

                for idx, field in zip(target_indices, target_fields):
                    try:
                        dd = dl_elements[idx].find_element("css selector", "dd").text.strip()
                        result[field] = dd
                    except Exception as e:
                        result[field] = None  # ê°’ì´ ì—†ì„ ê²½ìš° None ì²˜ë¦¬

                print(result)
            except Exception as e:
                print("êµ¬ë§¤ì •ë³´ íƒ­ í´ë¦­ ì‹¤íŒ¨:", e)

            # ë¦¬ë·°ì •ë³´ í´ë¦­ ë° ìˆ˜ì§‘
            try:
                sb.click("a.goods_reputation")
                sleep(2)
                print("âœ… ë¦¬ë·° ì •ë³´ íƒ­ í´ë¦­ ì™„ë£Œ")
                # ë¦¬ë·°ì •ë¦¬
                totalComment = sb.get_text("div.grade_img em")
                # ë¦¬ë·°ê°¯ìˆ˜
                numOfReviews = sb.get_text("div.star_area em")
                # ë¦¬ë·° í‰ì 
                avgReview = sb.get_text("div.star_area strong")
                # ë¦¬ë·° ì ìˆ˜ í¼ì„¼íŠ¸
                percent_elements = sb.find_elements("css selector", "ul.graph_list span.per")
                percent_list = [el.text.strip() for el in percent_elements]
                pctOf5 = percent_list[0]
                pctOf4 = percent_list[1]
                pctOf3 = percent_list[2]
                pctOf2 = percent_list[3]
                pctOf1 = percent_list[4]
            except Exception as e:
                print("âŒ ë¦¬ë·° ì •ë³´ íƒ­ í´ë¦­ ì™„ë£Œ:", e)
                # ë¦¬ë·° ì •ë³´
                polls = sb.find_elements("css selector", "dl.poll_type2.type3")
                review_detail = ""
                for poll in polls:
                    try:
                        # ì„¤ë¬¸ ì œëª© (ì˜ˆ: í”¼ë¶€íƒ€ì…)
                        title = poll.find_element("css selector", "span").text.strip()
                        review_detail = review_detail + "," + title

                        # í•˜ìœ„ í•­ëª©ë“¤ (li)
                        li_tags = poll.find_elements("css selector", "ul.list > li")
                        for li in li_tags:
                            label = li.find_element("css selector", "span.txt").text.strip()
                            percent = li.find_element("css selector", "em.per").text.strip()
                            review_detail = review_detail + "," + label + "," + percent
                        print(review_detail)
                    except Exception as e:
                        print("âŒ ì˜¤ë¥˜:", e)
            # ì €ì¥
            product_info = {
                "brand": brand_name,  # ë¸Œëœë“œëª…
                "product": product_name,  # ìƒí’ˆì´ë¦„
                "discountPrice": discount_price,  # í• ì¸ê°€
                "originPrice": origin_price,  # ì •ê°€
                "isPB": 1,  # Pbì—¬ë¶€
                "flag": flags,  # í˜œíƒ
                "totalcoment": totalComment,
                "numOfReviews": numOfReviews,
                "avgReview": avgReview,
                "pctOf5": pctOf5,
                "pctOf4": pctOf4,
                "pctOf3": pctOf3,
                "pctOf2": pctOf2,
                "pctOf1": pctOf1,
                "capacity": result['capacity'],
                "detail": result['detail'],
                "ingredients": result['ingredients']
            }
            product_data.append(product_info)
        from pprint import pprint
        pprint(product_data)
    return product_data

product_links = crawl_product_info()
crawl_product_detail(product_links)
